---
layout: article
title: "CS229T Lecture 2"
date: 2015-01-07T09:09:33-08:00
modified:
categories: lectures
excerpt: 
comments: true
mathjax:  true
tags: [machine learning]
ads: true
image:
  feature:
  teaser:
---


# Online Learning

Notations

$$
x \in \mathcal{X}, y \in \mathcal{Y}, x \in \mathcal{R}^d, y \in \{-1,1\}
$$

We see an input and output pair for each time step $t$.

We minimize the distance to the expert predictor.

$$
\begin{align*}
Regret(h) & = \sum_t^T \left[ l(y_t, p_t) - l(y_t, h(x_t)) \right] \\
Regret    & = \max_{h \in \mathcal{H}} Regret(h)
\end{align*}
$$

Objective is to minimize the $ Regret $. 

Example 1.
$\mathcal{H} = \\{ h_x, h_y \\} $
For, where each predictor only predicts either positive or negative 1 respectively.

With no assumptions, the $Regret$ is not sub-linear

$$ Regret \ge T/2 $$.

## Realizability

Assumption : realizable

$$
\exists h^\star \in \mathcal{H} \mbox{ such that } l(y_t, h^\star (x_t)) = 0, \forall t=1..T
$$


### Algorithm 1. (Majority Algorithm)

From a set of predictors, since we know there is the perfect predictor, we can find out the perfect predictor among a set of predictors after a finite time step assuming $ \|\mathcal{H}\| \le \infty $ from now on.

For each stage where majority vote fails, we can cross out at least half of predictors.

$$
Regret = M \le \log_2 |\mathcal{H}|
$$

The $Regret$ is bounded by a constant

# Convexity

For $\mathcal{S} \in \mathcal{R}^d, \mathcal{S} = \\{ w \in \mathcal{R}^d, \| w \|^2 \le B \\}$

Function $f$ is convex if there is some vector $z \in \mathcal{R}^d$ such that

$$
f(u) \ge f(w) + z (u - w) \mbox{ } \forall u \in $\mathcal{S}
$$

## Online Convex Optimization

For a series of time setps $t \in \[ 1 .. T\]$, the nature returns a function value $f_t : \mathcal{S} \rightarrow \mathcal{R}$



### Online linear regression

Problem: we have a online learner predicts a point and online convex optimizer which returns a variable to the online learner






