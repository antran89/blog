---
layout: article
title: "STATS 218 Lecture 3"
date: 2015-04-07T14:08:26-0700
modified:
categories: lectures
mathjax: true
excerpt: 
comments: true
mathjax:  true
tags: [statistics]
ads: true
image:
  feature:
  teaser:
---

## A Compound Poisson Distribution

$X_i$ i.i.d. cdf $F$ with mean $\mu$

independent of $X_i$ and $N \sim Poisson(\lambda)$ Let $W = \sum_{i=1}^N X_i$ then $W$ is said to have compound Poisson distribution.

$$
\begin{align}
EW & = E[E[W|N]]\\
& = E[\mu N]\\
& = \mu \lambda
\end{align}
$$

Since each $X_i$ has the mean of $\mu$.

### Moment Generating Function of $W$

$\phi_W(t) = E e^{tW}$

We can calculate $E(W^\alpha) = \phi_W^{(\alpha)}(0)$ where $\cdot^{(\alpha)}$ is $\alpha$ th derivative.

Coming back to the original problem. By conditioning on the number $N$, we get

$$
\begin{align}
\phi_W(t) & = E e^{tW} \\
& = E[ E[ e^{t \sum X_i} | N ]]\\
& = E[ \phi_X^N (t)]
\end{align}
$$

Thus,

$$
\begin{align}
E(W^\alpha) & = \sum_n^\infty \phi_X^n(t) \frac{e^{-\lambda}\lambda^n}{\lambda !} \\
& = e^{-\lambda} e^{\lambda \phi_X(t)}
\end{align}
$$

The mean and variance of the R.V $W$ can be easiliy computed by taking derivative of the M.G.F.

$$
\begin{align}
\frac{d}{dt} \phi(t) & = \exp{\lambda [\phi_x(t) - 1]} \lambda \phi_X’(t)
\end{align}
$$

The M.G.F of a Poisson R.V. can be easily computed by setting $X_i = 1$ and  $\phi_X(t) = e^t$.


## Renewal (or Counting) Processes

$X_i$ s i.i.d. and non-negative R.V.s with cdf $F$. (think “failure times”)

$S_0 = 0$ and $S_n = \sum_{i = 1}^nX_i$ = times until $n$ th failure.

Let $N(t)=$ number of events (or failure) by time $t$ (and including $t$).

<div class="notice">
Def: any random function $N(t)$ satisfying<br>

	- $N(t) \ge 0$<br>
	- $N(t)$ is  integer-valued<br>
	- $N(s) \le N(t) $ if $s \le t$</br>
	- ifd $s \lt t$, $N(t) - N(s)$ is the number of events occuring in $(s,t]$<br>

is a counting process
</div>


<div class="notice">
Def: A counting process is <br>

	- said to have independent incerements if number of events occuring in disjoint intervals are independent<br>
	- said to have stationary increments if the distribution of number of events in any interval only depends on its length.<br>
</div>

<div class="notice">
Def 1: $N(t)$ is a Poisson process with rate $\lambda$ where $\lambda \gt 0)$ if <br>

	- $N(0) = 0$<br>
	- process has independent increments<br>
	- number of events in $(s,t] \sim Poisson( \lambda( t- s))$<br>
</div>

The definition provided is not really constructive. A more general way to define a Poisson process is

$X_i$ i.i.d. $exponential(\lambda)$i.e. density $\lambda e^{-\lambda t}$.

$N(t) = $ number of failure by time $t$. 

f events that have happened by time $t$.

### Lecture 3

<div class="notice">
Def 2: $N(t)$ is a Poisson process with rate $\lambda$ if<br>
- $N(0) = 0$<br>
- Process has independent stationary increments<br>
- $P(N(h) = 1) = \lambda h + \alpha(h)$ as $h \rightarrow 0$<br>
- $P(N(h)>1) = o(h)$ as $h \rightarrow 0$ i.e. $\frac{P(N(h) > 1)}{h} \rightarrow 0$ as $h \rightarrow 0$<br>
- $P(N(h) = 1) = e^{-\lambda h} \lambda h=\left( 1 - \lambda h + \frac{(\lambda h)^2}{2} - ...\right) \lambda h$<br>
</div>

<div class="notice">
Def 3: Let $X_1, X_2, ...$ be i.i.d. $exponential(\lambda)$ random variables. i.e. density $\lambda e^{-\lambda t}$ where $t > 0$<br>
Let $S_n = \sum_{i = 1}^n X_i$. $N(t) = $ number of events by time $t$ is Poisson process with rate $\lambda$.<br>
</div>

An exponential random variable has a memoryless property: $P(X > t + s \| X > s) = P(X > t)$ for $\forall s,t$.

$$
\begin{align}
P(X > t+ s | X > s) & = \frac{ P(X > t + s, X > s)}{P(X > s )}\\
& = \frac{\bar{F}(t + s)}{\bar{F}(s)} \\
& = \frac{e^{-\lambda(t + s)}}{e^{-\lambda s}} = e^{-\lambda t}
\end{align}
$$

<div class="notice">
Survival function : $\bar{F}(x) = 1 - F(x)$ where $F(x)$ is a c.d.f.
</div>


<div class="notice-info">
Fact: If $X_i \sim exponential(\lambda)$<br>
Then, $S_n = \sum_{i = 1}^n X_i \sim Gamma(n,\lambda)$ with c.d.f.<br>
$$
G_{n,\lambda}(t) = P(S_n \le t) = 1 - e^{-\lambda t} \left(1 + \frac{\lambda t}{-1!} + ... + \frac{(\lambda t)^{n-1}}{(n-1)!} \right)
$$
</div>

Key identity relationship waiting time and dthe number of events $N(t)$

$$
\begin{align}
N(t) \ge n & \longleftrightarrow S_n \le t\\
P(N(t) \ge n) & = P(S_n \le t)\\
P(N(t) = n) & = P(S_n \le t) - P(S_{n+ 1} \le t)\\
& = G_{n, \lambda}(t) - G_{n+1, \lambda}(t)\\
& = e^{- \lambda t} (\lambda t)^n / n!
\end{align}
$$

e.g. Customers arrive at bakery $i$th customer with $Poisson(\lambda t)$ and order $Y_i$ pastries. $Y_i$'s are i.i.d. mean $\mu$ and independent of times ordered.
Let $M(t) = $ number of items purchased by time $t$

$$
\begin{align}
M(t) & = \sum_{i = 1}^{N(t)} Y_i\\
EM(t) & = E[ E[M(t) | N(t)]] = E[\mu N(t)] = \mu \lambda t
\end{align}
$$

e.g. Find the conditional distribution of $X$ given $N(t) = 1$.

$X \in [0,t)$ then

$$
\begin{align}
P(X < x | N(t) = 1) & = \frac{X_1 \le x, N(t) = 1}{P(N(t) = 1)}\\
& = \frac{P(N(x) = 1)P(N(t - x) = 2))}{e^{-\lambda t}\frac{\lambda t)}{1!}}\\
& = \frac{e^{-\lambda x} \lambda x e^{-\lambda(t- x)}}{e^{-\lambda t} \lambda t}\\
& = x /t
\end{align}
$$

i.e. $Uniform(0,t)$.


e.g. Conditional on $N(t) = n$, find the chance of no events after $t - \epsilon$.

$$
\begin{align}
& = \frac{P(n \text{ events in } [0, t - \epsilon) \text{ and } 0 \text{ events in } (t - \epsilon, t)))}{P(n \text{ events in } (0,t))}\\
& = \frac{\frac{e^{-\lambda ( t - \epsilon)} [\lambda(t - \epsilon)]^n}{n!} e^{-\lambda \epsilon}}{\frac{e^{-\lambda t}(\lambda t)^n}{n!}}\\
& = \left( \frac{t - \epsilon}{t}\right)^n
\end{align}
$$

<div class="notice-info">
General Fact: conditional on $N(t) = n$, then the placement of the times of $n$ events $(S_1, S_2, ..., S_n)$ has the same joint distribution as $(U_{(1)}, ..., U_{(n)})$ where $U-i$ i.i.d. $Unif(0,t)$, $U_{(1)} \le U_{(2)} \le ... \le U_{(n)}$ are the ordered $U_1, ..., U_n$. 
</div>

e.g. Customers arrive according to a Poisson process $\lambda$ at a train deposit. The train departs at time $t$. Let $W = $ total waiting time of the passengers who enter the train.

$$
\begin{align}
W & = \sum_{i = 1}^{N(t)} t - S_i\\
EW & =E[E[\sum_{i = 1}^{N(t)} (t - U_{(i)})|N(t)]]\\
& = E [t N(t)] - E[E[\sum_{i = 1}^{N(t)} U_{(i)}|N(t)]]\\
& = E [t N(t)] - E[E[\sum_{i = 1}^{N(t)} U_{i}|N(t)]]\\
& = \lambda t^2 - E[t/2 N(t)]\\
& = \lambda t^2 /2
\end{align}
$$

Arrival according to $Poisson(\lambda)$, If an event happens at time $s$, then independent of everything else, it is Type 1 with probability $P(s)$
Type 2 with $1 - P(s)$
Let $N_i(t) = $ number of type $i$ arrival by time $t$ ($N = N_1 + N_2)$.

<div class="notice-success">
Fact: $N_1(t)$ and $N_2(t)$ are independent Poisson with mean $\lambda t p $ and $\lambda t (1 - p)$ respectively where $p = 1/t \int_0^t P(s) ds$.
<br>
$$
\begin{align}
P(N_1(t) = n, N_t(t) = m) & = P(N_1(t) = n, N_2(t)  = m | N(t) = n + m) P(N(t) = n + m)\\
& = \left( \begin{array}{c} n + m \\ n \end{array} \right) p^n (1- p)^m e^{-\lambda t} (\lambda t)^{n + m} / (n + m)!\\
& = e^{-\lambda t p } (\lambda t p )^n / n!  e^{-\lambda t (1-p) } (\lambda t (1- p) )^m / m!
\end{align}
$$
</div>
