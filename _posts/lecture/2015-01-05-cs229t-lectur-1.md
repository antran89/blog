---
layout: article
title: "CS229T Lecture 1"
date: 2015-01-05T09:09:32-08:00
modified:
categories: lecture_notes
excerpt: 
comments: true
mathjax:  true
tags: [vim]
ads: true
image:
  feature:
  teaser:
---


# CS229T Overview

## Online Learning 

Collect data on-the-fly and train the classifier accordingly for incoming dataset. Suppose one classifier.

Let $P^\star$ be the expert classifier. Then we define the number of smaple in the dataset be $T$.

Instead of minimizing the loss $l$ directly, we can minimize the distance to the expert predictor.

$$
Regret = \sum_t^T | l(y_t, Pt) - l(y_t, P^\star(x_t)) |\\
$$

The $Regret$ satisfy the following inequality.

$$ 
Regret \le O(\sqrt{T \log |\mathcal{H}})
$$

## Uniform Convergence

The generalization error is bounded by the complexity of predictor $\mathcal{H}$ and the training error.

$$
L_{test}(\hat{h}) \le L(\hat{h}) + O(\sqrt{\frac{complexity(H)}{T}})
$$

## Direct Method

## Kernel Method

Defining the hypothesis and finding the right representation are very difficult. One can engineer features to work better but instead of designing feature, one can measure the notion of similarity in the kernel space by defining $k(x,xâ€™)$.


## Latent Variable Model


# Online Learning

Notations

$$
x \in \mathcal{X}, y \in \mathcal{Y}, x \in \mathcal{R}^d, y \in \{-1,1\}
$$

We see an input and output pair for each time step $t$.

We minimize the distance to the expert predictor.

$$
\begin{align*}
Regret(h) & = \sum_t^T \left[ l(y_t, p_t) - l(y_t, h(x_t)) \right] \\
Regret    & = \max_{h \in \mathcal{H}} Regret(h)
\end{align*}
$$

Objective is to minimize the $ Regret $. 

Example 1.
$\mathcal{H} = \\{ h_x, h_y \\} $
For, where each predictor only predicts either positive or negative 1 respectively.

Then the

$$ Regret \ge T/2 $$.

> Assumption (realizable)

$$
\exists h^\star \in \mathcal{H} \mbox{ such that } l(y_t, h^\star (x_t)) = 0, \forall t=1..T
$$

From a set of predictors, since we know there is the perfect predictor, we can find out the perfect predictor among a set of predictors after a finite time step assuming $ \|\mathcal{H}\| \le \infty $ from now on.

## Algorithm 1. (Majority Algorithm)

The

$$
Regret = M \le \log_2 |\mathcal{H}|
$$
