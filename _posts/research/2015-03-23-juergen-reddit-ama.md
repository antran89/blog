---
layout: article
title: "Jurget Schmidhuber’s thoughts on RNN"
date: 2015-03-23T23:41:01-0700
modified:
categories: research
excerpt:
comments: true
tags:
toc: true
ads: true
image:
  feature:
  teaser:
---

## Jurgen Schmidhuber answers on Reddit

[FastML Summary](http://fastml.com/juergen-schmidhuber-s-answers-from-the-reddit-ama/)

[Jurgen’s Theory of Universal AI](http://people.idsia.ch/%7Ejuergen/unilearn.html)

[Jurgen’s Deep Learning Book](http://people.idsia.ch/%7Ejuergen/deep-learning-overview.html)

## Where do LSTMs originate from

Q: How on earth did you and Hochreiter come up with LSTM units? They seem radically more complicated than any other “neuron” structure I’ve seen, and everytime I see the figure, I’m shocked that you’re able to train them.

A: In my first Deep Learning project ever, Sepp Hochreiter (1991) analysed the vanishing gradient problem. LSTM falls out of this almost naturally :-)

